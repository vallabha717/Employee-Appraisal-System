# -*- coding: utf-8 -*-
"""Assignment 2: Sentimental Analysis on IMDB Rating.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WG-gR0PEQiJ7uyHLBawlJgaUlPyB2Z_8
"""

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense

vocab_size = 10000
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

maxlen = 200
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

#Lstm Code.
model_lstm = Sequential([
    Embedding(vocab_size, 128, input_length=maxlen),
    LSTM(64),
    Dense(1, activation='sigmoid')
])
model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#Gru Code.
model_gru = Sequential([
    Embedding(vocab_size, 128, input_length=maxlen),
    GRU(64),
    Dense(1, activation='sigmoid')
])
model_gru.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

from tensorflow.keras.layers import Bidirectional

model_bilstm = Sequential([
    Embedding(vocab_size, 128, input_length=maxlen),
    Bidirectional(LSTM(64)),
    Dense(1, activation='sigmoid')
])
model_bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#Bidirectional LSTM Code.
model_bilstm = Sequential([
    Embedding(vocab_size, 128, input_length=maxlen),
    Bidirectional(LSTM(64)),
    Dense(1, activation='sigmoid')
])
model_bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history_lstm = model_lstm.fit(x_train, y_train, epochs= 10, batch_size=128, validation_split=0.2)
history_gru = model_gru.fit(x_train, y_train, epochs= 10, batch_size=128, validation_split=0.2)
history_bilstm = model_bilstm.fit(x_train, y_train, epochs= 10, batch_size=128, validation_split=0.2)

print("LSTM Accuracy:", model_lstm.evaluate(x_test, y_test)[1])
print("GRU Accuracy:", model_gru.evaluate(x_test, y_test)[1])
print("BiLSTM Accuracy:", model_bilstm.evaluate(x_test, y_test)[1])

plt.plot(history_lstm.history['val_accuracy'], label='LSTM')
plt.plot(history_gru.history['val_accuracy'], label='GRU')
plt.plot(history_bilstm.history['val_accuracy'], label='BiLSTM')
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

plt.plot(history_lstm.history['val_accuracy'], label='LSTM')
plt.plot(history_gru.history['val_accuracy'], label='GRU')
plt.plot(history_bilstm.history['val_accuracy'], label='BiLSTM')
plt.title('Validation Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.grid()
plt.show()

print("Test Accuracy of LSTM:", model_lstm.evaluate(x_test, y_test)[1])
print("Test Accuracy of GRU:", model_gru.evaluate(x_test, y_test)[1])
print("Test Accuracy of BiLSTM:", model_bilstm.evaluate(x_test, y_test)[1])

# Get word index
word_index = imdb.get_word_index()
reverse_word_index = {value: key for (key, value) in word_index.items()}

# Example: Convert custom review to sequence
def encode_review(text):
    tokens = [1]  # start token
    for word in text.lower().split():
        idx = word_index.get(word, 2)  # 2 = unknown
        tokens.append(idx)
    return pad_sequences([tokens], maxlen=200)

sample_review = "this movie was amazing with great acting and direction"
encoded = encode_review(sample_review)

# Predict using BiLSTM
prediction = model_bilstm.predict(encoded)
print("Sentiment Score (0 = neg, 1 = pos):", prediction[0][0])

model_bilstm.summary()
model_lstm.summary()
model_gru.summary()

